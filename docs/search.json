[
  {
    "objectID": "dagnammit.html#about-me",
    "href": "dagnammit.html#about-me",
    "title": "DAG Nammit",
    "section": "About me",
    "text": "About me\n\n\n\nüë®‚Äçüéì B.S. & M.S. in Applied Statistics (WSU)\nüë®‚Äçüéì Ph.D.¬†in Applied Matehmatics (AFIT)\nüé¨ Director, Data Science at 84.51Àö\nüë®‚Äçüè´ UC LCB adjunct (~7 years)\nSome R packages üì¶ :\n\npdp (partial dependence plots)\nvip (variable importance plots)\nfastshap (faster SHAP values)\n\nSome books üìö :\n\nHands-On Machine Learning with R\nTree-Based Methods for Statistical Learning"
  },
  {
    "objectID": "dagnammit.html#why-does-explainability-matter",
    "href": "dagnammit.html#why-does-explainability-matter",
    "title": "DAG Nammit",
    "section": "Why does explainability matter?",
    "text": "Why does explainability matter?\n\n\n\nModel debugging - Why did my model Netflix make this mistake?\nFeature Engineering - How can I improve my model?\nDetecting fairness issues - Does my model discriminate?\nHuman-AI cooperation - How can I understand and trust the model‚Äôs decisions?\nRegulatory compliance - Does my model satisfy legal requirements?\nHigh-risk applications - Healthcare, finance, judicial, ‚Ä¶\nCommon sense"
  },
  {
    "objectID": "dagnammit.html#interpretability-in-a-nutshell",
    "href": "dagnammit.html#interpretability-in-a-nutshell",
    "title": "DAG Nammit",
    "section": "Interpretability in a nutshell ü•ú",
    "text": "Interpretability in a nutshell ü•ú\n\n‚ÄúInterpretability ‚äÇ Explainability‚Äù\nGlobal vs.¬†local explainability\n‚óºÔ∏èBlack-box vs.¬†ü™üglass-box models\n‚ö†Ô∏è Model-agnostic ‚ö†Ô∏è vs.¬†model-specific techniques\nMulticollinearity is the nemesis of interpretability!\nüßë‚Äçüíª Lots of good software!\n\n\n\n\n\nüëâ"
  },
  {
    "objectID": "dagnammit.html#useful-resources",
    "href": "dagnammit.html#useful-resources",
    "title": "DAG Nammit",
    "section": "Useful resources",
    "text": "Useful resources"
  },
  {
    "objectID": "dagnammit.html#so-whats-the-problems-with-causally-interpreting-machine-learning-models",
    "href": "dagnammit.html#so-whats-the-problems-with-causally-interpreting-machine-learning-models",
    "title": "DAG Nammit",
    "section": "So what‚Äôs the problem(s) with causally interpreting machine learning models?",
    "text": "So what‚Äôs the problem(s) with causally interpreting machine learning models?\n\n\n\n\n\n\n\nImportant\n\n\nMachine learning is often applied to observational or happenstance data!"
  },
  {
    "objectID": "dagnammit.html#correlation-doesnt-imply-causation",
    "href": "dagnammit.html#correlation-doesnt-imply-causation",
    "title": "DAG Nammit",
    "section": "Correlation doesn‚Äôt imply causation üôÑ",
    "text": "Correlation doesn‚Äôt imply causation üôÑ\n\n‚Äú‚Ä¶but this does not necessarily stop people from drawing causal inferences from correlational staements.‚Äù\n\n\n\n\nSome common causal fallacies:\n\nPost hoc ergo propter hoc (i.e., ‚Äúrooster syndrome‚Äù)\n\nDoes the rooster‚Äôs crowing cause the sun to rise?\nCoincidental vaccine adverse events\n\nCum hoc ergo propter hoc\n\nShoe size and reading ability\nIce cream sales and shark attacks\nSpurious correlations by Tyler Vigen"
  },
  {
    "objectID": "dagnammit.html#some-causal-fallacies-in-the-wild",
    "href": "dagnammit.html#some-causal-fallacies-in-the-wild",
    "title": "DAG Nammit",
    "section": "Some causal fallacies in the wild",
    "text": "Some causal fallacies in the wild"
  },
  {
    "objectID": "dagnammit.html#customer-retention-example",
    "href": "dagnammit.html#customer-retention-example",
    "title": "DAG Nammit",
    "section": "Customer retention example",
    "text": "Customer retention example\n\nInitial goal is to train a model to predict whether a customer will renew their software subscription (taken from Lundberg et al.¬†(2021))\nEight features were identified for predicting retention (Did.renew=0/1):\n\nCustomer discount offered upon renewal (Discount)\nAd spending on this type of customer since last renewal (Ad.spend)\nCustomer‚Äôs monthly usage (Monthly.usage)\nTime since last upgrade upon renewal (Last.upgrade)\nNo.¬†bugs reported by customer since last renewal (Bugs.reported)\nNo.¬†interactions with customer since last renewal (Interactions)\nNo.¬†sales calls with customer since last renewal (Sales.calls)\nHealth of regional economy upon renewal (Economy)\n\n10k total records: 8k for training and 2k for validation"
  },
  {
    "objectID": "dagnammit.html#retention-example-cont.",
    "href": "dagnammit.html#retention-example-cont.",
    "title": "DAG Nammit",
    "section": "Retention example (cont.)",
    "text": "Retention example (cont.)\nOutput from an additive logistic regression fit:\n\n\n              Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept)     -0.665      0.134  -4.961    0.000\nSales.calls      0.074      0.060   1.238    0.216\nInteractions     0.091      0.056   1.612    0.107\nEconomy          0.597      0.091   6.589    0.000\nLast.upgrade    -0.022      0.005  -4.190    0.000\nDiscount        -5.950      0.311 -19.106    0.000\nMonthly.usage    0.351      0.146   2.406    0.016\nAd.spend         0.602      0.062   9.766    0.000\nBugs.reported    0.259      0.035   7.345    0.000\n\n\n\n\n\n\n\n\nTip\n\n\nChecking variance inflation factors (VIFs) is always a good idea, even for black-box models!"
  },
  {
    "objectID": "dagnammit.html#retention-example-cont.-1",
    "href": "dagnammit.html#retention-example-cont.-1",
    "title": "DAG Nammit",
    "section": "Retention example (cont.)",
    "text": "Retention example (cont.)\nPearson correlation matrix:"
  },
  {
    "objectID": "dagnammit.html#retention-example-cont.-2",
    "href": "dagnammit.html#retention-example-cont.-2",
    "title": "DAG Nammit",
    "section": "Retention example (cont.)",
    "text": "Retention example (cont.)\nVariable importance scores from an XGBoost fit:"
  },
  {
    "objectID": "dagnammit.html#partial-dependence-pd-plots",
    "href": "dagnammit.html#partial-dependence-pd-plots",
    "title": "DAG Nammit",
    "section": "Partial dependence (PD) plots",
    "text": "Partial dependence (PD) plots"
  },
  {
    "objectID": "dagnammit.html#interpreting-the-pd-plots",
    "href": "dagnammit.html#interpreting-the-pd-plots",
    "title": "DAG Nammit",
    "section": "Interpreting the PD plots",
    "text": "Interpreting the PD plots\n\nAd.spend and Discount are important to this (fictional) business because they can be directly manipulated üéõÔ∏è\n\n\n\nüôåüéâü•≥ Hurrah! We can improve retention by\n\n‚¨ÜÔ∏è Increasing ad spend\n‚¨áÔ∏è Decreasing discount amount\n\n\n\n\n\nNOT SO FAST!!!"
  },
  {
    "objectID": "dagnammit.html#the-true-data-generator",
    "href": "dagnammit.html#the-true-data-generator",
    "title": "DAG Nammit",
    "section": "The true data generator",
    "text": "The true data generator\n\\[\n\\begin{aligned}\n\\mathsf{logit}\\left(p\\right) = 1.26 &\\times \\mathtt{Product.need} + \\\\\n0.56 &\\times\\mathtt{Monthly.usage} + \\\\\n0.7 &\\times \\mathtt{Economy} + \\\\\n0.35 &\\times \\mathtt{Discount} + \\\\\n0.35 &\\times \\left(1 - \\mathtt{Bugs.faced} / 20\\right) + \\\\\n0.035 &\\times \\mathtt{Sales.calls} + \\\\\n0.105 &\\times \\mathtt{Interactions} + \\\\\n0.7 &\\times \\left(\\mathtt{Last.upgrade} / 4 + 0.25\\right)^{-1} + \\\\\n0 &\\times \\mathtt{Ad.spend} + \\\\\n&-3.15 + \\epsilon\\\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "dagnammit.html#partial-dependence-vs.-truth",
    "href": "dagnammit.html#partial-dependence-vs.-truth",
    "title": "DAG Nammit",
    "section": "Partial dependence vs.¬†truth! üò±",
    "text": "Partial dependence vs.¬†truth! üò±\nPD plot (black) vs.¬†true causal relationship (red)"
  },
  {
    "objectID": "dagnammit.html#even-the-experts-slip-up",
    "href": "dagnammit.html#even-the-experts-slip-up",
    "title": "DAG Nammit",
    "section": "Even the experts slip up!",
    "text": "Even the experts slip up!\n\nStatistical Learning with Big Data (fantastic talk!)"
  },
  {
    "objectID": "dagnammit.html#so-now-what",
    "href": "dagnammit.html#so-now-what",
    "title": "DAG Nammit",
    "section": "So now what?",
    "text": "So now what?\n\n\n\nCausal interpretation requires a causal model!!\n\n\n\n\n\n\n\n\n\n\nüì∫ Watch the first talk by Peter Tennant!\n\n\nInterpretable Machine Learning & Causal Inference Workshop"
  },
  {
    "objectID": "dagnammit.html#directed-asyclic-graphs-dags",
    "href": "dagnammit.html#directed-asyclic-graphs-dags",
    "title": "DAG Nammit",
    "section": "Directed asyclic graphs (DAGs)",
    "text": "Directed asyclic graphs (DAGs)\n\nUseful for representing causal relationships and assumptions\n\nDirected: One-sided arrows (‚Üí) connect (assumed) causes and effects\nAsyclic: no directed path can form a closed loop\n\nHelp determine whether the effect(s) of interest can be estimated from available data\nBased on strong assumptions that are often unverifiable"
  },
  {
    "objectID": "dagnammit.html#dags-in-machine-learning",
    "href": "dagnammit.html#dags-in-machine-learning",
    "title": "DAG Nammit",
    "section": "DAGs in machine learning",
    "text": "DAGs in machine learning\nAssume we have five features (X1‚ÄìX5) and a response (Y). Causally interpreting a machine learning model assumes a very particular DAG!\n\n\nHow your algorithm sees it:\n\n\n\n\n\nflowchart TB\n  X1 --&gt; Y\n  X2 --&gt; Y\n  X3 --&gt; Y\n  X4 --&gt; Y\n  X5 --&gt; Y\n\n\n\n\n\n\n\nHow the universe works:\n\n\n\n\n\nflowchart TB\n  X1 --&gt; X3\n  X1 --&gt; Y\n  X2 --&gt; X3\n  X2 --&gt; Y\n  X3 --&gt; X4\n  X3 --&gt; Y\n  X4 --&gt; Y\n  X5 --&gt; Y"
  },
  {
    "objectID": "dagnammit.html#estimation-and-confounding",
    "href": "dagnammit.html#estimation-and-confounding",
    "title": "DAG Nammit",
    "section": "Estimation and confounding",
    "text": "Estimation and confounding\n\nIn causal inference, a common goal is to estimate the average (caual) effect of some ‚Äútreatment‚Äù on an outcome of interest (e.g., effect of an ad campaign on sales)\n\nWhen the treatment is binary, the effect estimate is referred to as the average treatment effect (ATE)\n\nEstimation typically requires adjusting (and not adjusting) for certain variables\nA confounder is a variable that effects both the treatment and outcome\n\nConfounders must be identified, measured, and appropriately adjusted for in the analysis\n\nNeed to be careful with other covariate roles, like colliders, mediators, etc."
  },
  {
    "objectID": "dagnammit.html#adjustment-sets-are-key",
    "href": "dagnammit.html#adjustment-sets-are-key",
    "title": "DAG Nammit",
    "section": "Adjustment sets are key üîë",
    "text": "Adjustment sets are key üîë\n\n\nMinimal sufficient adjustment set for estimating\n\nTotal effect of X3 on Y: {X1, X2}\nDirect effect of X3 on Y: {X1, X2, X4}\n\n\n\n\n\n\n\nflowchart TB\n  X1 --&gt; X3\n  X1 --&gt; Y\n  X2 --&gt; X3\n  X2 --&gt; Y\n  X3 --&gt; X4\n  X3 --&gt; Y\n  X4 --&gt; Y\n  X5 --&gt; Y\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\nTools like DAGitty can help automate this!"
  },
  {
    "objectID": "dagnammit.html#copy-and-paste-this-code-into-dagitty",
    "href": "dagnammit.html#copy-and-paste-this-code-into-dagitty",
    "title": "DAG Nammit",
    "section": "Copy and paste this code into DAGitty",
    "text": "Copy and paste this code into DAGitty\ndag {\nbb=\"0,0,1,1\"\nX1 [pos=\"0.462,0.332\"]\nX2 [pos=\"0.425,0.238\"]\nX3 [exposure,pos=\"0.532,0.277\"]\nX4 [pos=\"0.529,0.396\"]\nX5 [pos=\"0.363,0.416\"]\nY [outcome,pos=\"0.439,0.464\"]\nX1 -&gt; X3\nX1 -&gt; Y\nX2 -&gt; X3\nX2 -&gt; Y\nX3 -&gt; X4\nX3 -&gt; Y\nX4 -&gt; Y\nX5 -&gt; Y\n}"
  },
  {
    "objectID": "dagnammit.html#useful-resources-1",
    "href": "dagnammit.html#useful-resources-1",
    "title": "DAG Nammit",
    "section": "Useful resources",
    "text": "Useful resources"
  },
  {
    "objectID": "dagnammit.html#retention-example-cont.-3",
    "href": "dagnammit.html#retention-example-cont.-3",
    "title": "DAG Nammit",
    "section": "Retention example (cont.)",
    "text": "Retention example (cont.)\nAssume strong domain expertise has allowed us to generate the following DAG:\n\n\n\n%3\n\n\n\nBugs reported\n\nBugs reported\n\n\n\nMonthly usage\n\nMonthly usage\n\n\n\nAd spend\n\nAd spend\n\n\n\nMonthly usage-&gt;Ad spend\n\n\n\n\n\nBugs faced\n\nBugs faced\n\n\n\nMonthly usage-&gt;Bugs faced\n\n\n\n\n\nDid renew\n\nDid renew\n\n\n\nMonthly usage-&gt;Did renew\n\n\n\n\n\nSales calls\n\nSales calls\n\n\n\nInteractions\n\nInteractions\n\n\n\nSales calls-&gt;Interactions\n\n\n\n\n\nProduct need\n\nProduct need\n\n\n\nSales calls-&gt;Product need\n\n\n\n\n\nSales calls-&gt;Did renew\n\n\n\n\n\nEconomy\n\nEconomy\n\n\n\nEconomy-&gt;Did renew\n\n\n\n\n\nDiscount\n\nDiscount\n\n\n\nDiscount-&gt;Did renew\n\n\n\n\n\nLast upgrade\n\nLast upgrade\n\n\n\nLast upgrade-&gt;Ad spend\n\n\n\n\n\nLast upgrade-&gt;Did renew\n\n\n\n\n\nInteractions-&gt;Did renew\n\n\n\n\n\nProduct need-&gt;Bugs reported\n\n\n\n\n\nProduct need-&gt;Monthly usage\n\n\n\n\n\nProduct need-&gt;Discount\n\n\n\n\n\nProduct need-&gt;Did renew\n\n\n\n\n\nBugs faced-&gt;Bugs reported\n\n\n\n\n\nBugs faced-&gt;Did renew"
  },
  {
    "objectID": "dagnammit.html#mathematical-background",
    "href": "dagnammit.html#mathematical-background",
    "title": "DAG Nammit",
    "section": "Mathematical background",
    "text": "Mathematical background\nThe partial dependence (PD) of \\(Y\\) on \\(X_S\\) is defined as\n\\[\n\\begin{aligned}\ng_s\\left(x_s\\right) &= E_{X_c}\\left[g\\left(x_s, X_c\\right)\\right] \\\\\n&\\approx \\frac{1}{N}\\sum_{i=1}^N g\\left(x_S, X_{iC}\\right)\n\\end{aligned}\n\\]\n\n\nThis is the same as Pearl‚Äôs back-door adjustment formula\nUnder certain conditions, PD plots can be interpreted causally!"
  },
  {
    "objectID": "dagnammit.html#retention-example-cont.-4",
    "href": "dagnammit.html#retention-example-cont.-4",
    "title": "DAG Nammit",
    "section": "Retention example (cont.)",
    "text": "Retention example (cont.)\nPD of Did.renew on Ad.spend, adjusted for only Monthly.usage and Last.upgrade: ‚Ä¶ ü•Å\n\n\n\n\n\n\n\n\n\n\n\n\nUmmm ‚Ä¶ maybe a case of estimand vs.¬†estimate? ü§î"
  },
  {
    "objectID": "dagnammit.html#stop-permuting-features",
    "href": "dagnammit.html#stop-permuting-features",
    "title": "DAG Nammit",
    "section": "Stop permuting features?! üò±",
    "text": "Stop permuting features?! üò±\n\n\n‚Ä¶PaP metrics can vastly over-emphasize correlated features in both variable importance measures and partial dependence plots."
  },
  {
    "objectID": "dagnammit.html#retention-example-cont.-5",
    "href": "dagnammit.html#retention-example-cont.-5",
    "title": "DAG Nammit",
    "section": "Retention example (cont.)",
    "text": "Retention example (cont.)"
  },
  {
    "objectID": "dagnammit.html#doubledebiased-machine-learning-dml",
    "href": "dagnammit.html#doubledebiased-machine-learning-dml",
    "title": "DAG Nammit",
    "section": "Double/debiased machine learning (DML)",
    "text": "Double/debiased machine learning (DML)\n\\[\n\\begin{align}\n  y &= \\theta t + g\\left(\\boldsymbol{x}\\right) + U, \\quad E\\left(U|t, \\boldsymbol{x}\\right) = 0\\\\\n  t &= m\\left(\\boldsymbol{x}\\right) + V, \\quad E\\left(V|\\boldsymbol{x}\\right) = 0\n\\end{align}\n\\]\nDML ‚ö†Ô∏èessentially‚ö†Ô∏è involves three steps:\n\nPredict the outcome (\\(y\\)) from an appropriate adjustment set (\\(\\boldsymbol{x}\\)) and get the residuals (\\(r_y\\))\nPredict the treatment (\\(t\\)) from the same adjustment set (\\(\\boldsymbol{x}\\)) and get the residuals (\\(r_x\\))\nRegress \\(r_y\\) on \\(r_x\\) to create a model of the average causal effect (i.e., the slope)"
  },
  {
    "objectID": "dagnammit.html#double-ml-for-ad.spend",
    "href": "dagnammit.html#double-ml-for-ad.spend",
    "title": "DAG Nammit",
    "section": "Double ML for Ad.spend",
    "text": "Double ML for Ad.spend\n\nR codeResults\n\n\n\ndml_data &lt;- DoubleML::DoubleMLData$new(\n  data = ret.trn,                              # training data\n  y_col = \"Did.renew\",                         # response\n  d_cols = \"Ad.spend\",                         # treatment\n  x_cols = c(\"Last.upgrade\", \"Monthly.usage\")  # adjustment set\n)\nlrnr &lt;- mlr3::lrn(\"regr.ranger\", num.trees = 500)\nset.seed(1810)  # for reproducibility\ndml_plr = DoubleML::DoubleMLPLR$new(\n  dml_data, ml_l = lrnr$clone(), ml_m = lrnr$clone()\n)\ndml_plr$fit()\n\n\n\n\n# Print results\nprint(dml_plr)\n# ------------------ Fit summary       ------------------\n#   Estimates and significance testing of the effect of target variables\n#          Estimate. Std. Error t value Pr(&gt;|t|)\n# Ad.spend  -0.09634    0.25197  -0.382    0.702\n\n# Compute 95% confidence interval\nprint(dml_plr$confint())\n#               2.5 %   97.5 %\n# Ad.spend -0.5901917 0.397511"
  },
  {
    "objectID": "dagnammit.html#yet-another-simulation-example",
    "href": "dagnammit.html#yet-another-simulation-example",
    "title": "DAG Nammit",
    "section": "Yet another simulation example‚Ä¶",
    "text": "Yet another simulation example‚Ä¶\nIn this example, we‚Äôll work with simulated data from the following causal model:\n\\[\\begin{equation*}\n\\log\\left(\\frac{p}{1 - p}\\right) = \\theta t + g\\left(\\boldsymbol{x}\\right),\n\\end{equation*}\\] where \\(p = Pr\\left(Y = 1 | t, \\boldsymbol{x}\\right)\\), \\(\\theta = 5\\) (i.e., the true causal effect of \\(t\\) on \\(Y\\)), and the nuisance functions are given by \\[\\begin{align*}\n  g\\left(\\boldsymbol{x}\\right) &= 10  \\sin\\left(\\pi x_1\\right) - 5  \\cos\\left(x_2\\right) + 20 x_3^2 + 10 x_4 + 5 x_5,\\\\\n  m\\left(\\boldsymbol{x}\\right)  &= 3 \\sin\\left(x_1\\right) + \\left(\\frac{1}{4}\\right)\\frac{\\exp\\left(x_2\\right)}{1 + \\exp\\left(x_2\\right)} - 2  x_3^2 + 2 x_4 + 2 x_5 = E\\left(t|\\boldsymbol{x}\\right),\n\\end{align*}\\]\n\nWalk through the python code in this notebook"
  },
  {
    "objectID": "dagnammit.html#designed-experiments",
    "href": "dagnammit.html#designed-experiments",
    "title": "DAG Nammit",
    "section": "Designed experiments",
    "text": "Designed experiments\n\nRCTs are arguably still the gold standard, but ‚Ä¶\n\nüòá There can be ethical concerns\nüí∞ Can be expensive to implement\n\n\nHowever‚Ä¶\n\n\n\n\n\n\n\nTip\n\n\nResponsible, transparent use of machine learning can help narrow down the hypothesis space!"
  },
  {
    "objectID": "dagnammit.html#ingot-cracking-example",
    "href": "dagnammit.html#ingot-cracking-example",
    "title": "DAG Nammit",
    "section": "Ingot cracking example",
    "text": "Ingot cracking example\nI‚Äôm reminded of an old (but still fantastic) data mining lecture from Richard De Veaux (skip to the 44:30 mark)\n\n20,000 lb. ingots made in a giant mold\nRoughtly 25% of ingots develop cracks\nCracked ingots cost $30,000 to recast\nRoughly 900 observations (ingots) on 149 variables\nWhat‚Äôs causing them to crack?"
  },
  {
    "objectID": "dagnammit.html#ingot-cracking-example-cont.",
    "href": "dagnammit.html#ingot-cracking-example-cont.",
    "title": "DAG Nammit",
    "section": "Ingot cracking example (cont.)",
    "text": "Ingot cracking example (cont.)\n\n\nLots of iterations, but‚Ä¶ ‚ÄúLooks like Chrome(!?)‚Äù\nüïµÔ∏è A glass-box model gave clues for generating a hypothesis (i.e., which variable to focus on)\nFollow-up randomized experiments led to substantial improvement!"
  },
  {
    "objectID": "dagnammit.html#adding-constraints-where-feasible",
    "href": "dagnammit.html#adding-constraints-where-feasible",
    "title": "DAG Nammit",
    "section": "Adding constraints (where feasible)",
    "text": "Adding constraints (where feasible)\n\nOften useful to constrain the functional form of the model in some way\n\nBusiness considerations\nDomain knowledge\n\nEnforcing sparsity (e.g., EBMs with Sparsity)\nEnforcing monotonicty between features and the predicted output can be done in several ways during training (e.g., linear and tree-based models)\n\nCan also be accomplished through model editing"
  },
  {
    "objectID": "dagnammit.html#pneumonia-example",
    "href": "dagnammit.html#pneumonia-example",
    "title": "DAG Nammit",
    "section": "Pneumonia example",
    "text": "Pneumonia example\n\nData contains 46 features on 14199 pneumonia patients\n\nPatient demographics (e.g., age)\nüìê Various measuremnts (e.g., heart rate)\nüî¨ Lab test results (e.g., WBC)\nü©ª Chest x-ray results (e.g., pleural effusion)\n\nGoal is to predict probability of death (0/1) using a GA2M\nData from Caruana et al.¬†(2015) and Wang et al.¬†(2022)"
  },
  {
    "objectID": "dagnammit.html#pneumonia-example-cont.",
    "href": "dagnammit.html#pneumonia-example-cont.",
    "title": "DAG Nammit",
    "section": "Pneumonia example (cont.)",
    "text": "Pneumonia example (cont.)\nLiving past 100 decreases risk? ü´§"
  },
  {
    "objectID": "dagnammit.html#pneumonia-example-cont.-1",
    "href": "dagnammit.html#pneumonia-example-cont.-1",
    "title": "DAG Nammit",
    "section": "Pneumonia example (cont.)",
    "text": "Pneumonia example (cont.)\nAdding monotonic constraints can be helpful!"
  },
  {
    "objectID": "dagnammit.html#pneumonia-example-cont.-2",
    "href": "dagnammit.html#pneumonia-example-cont.-2",
    "title": "DAG Nammit",
    "section": "Pneumonia example (cont.)",
    "text": "Pneumonia example (cont.)\nHaving asthma lowers a patient‚Äôs risk of dying from pneumonia? ü§Ø"
  },
  {
    "objectID": "dagnammit.html#pneumonia-example-cont.-3",
    "href": "dagnammit.html#pneumonia-example-cont.-3",
    "title": "DAG Nammit",
    "section": "Pneumonia example (cont.)",
    "text": "Pneumonia example (cont.)\nAccording to the doctors, asthmatic patients (A) would likely receive better care earlier (T):\n\n\n\n\n\n\n\nG\n\n\n\nA\n\nA\n\n\n\nR\n\nR\n\n\n\nA-&gt;R\n\n\n\n\n\nT\n\nT\n\n\n\nA-&gt;T\n\n\n\n\n\nT-&gt;R"
  },
  {
    "objectID": "dagnammit.html#pneumonia-example-cont.-4",
    "href": "dagnammit.html#pneumonia-example-cont.-4",
    "title": "DAG Nammit",
    "section": "Pneumonia example (cont.)",
    "text": "Pneumonia example (cont.)\n\nIf we use the model as is to make hospital admission decisions, asthmatic patients are likely to miss out on care they need\nInterpretability and causal knowledge can help identify such dangerous patterns and improve the model:\n\nForce monotonicity (e.g., asthmatic &gt; non-asthmatic)\nRemove the asthma feature\nEdit the effect out üò± (e.g., using GAM Changer)"
  },
  {
    "objectID": "dagnammit.html#gam-changer",
    "href": "dagnammit.html#gam-changer",
    "title": "DAG Nammit",
    "section": "GAM Changer",
    "text": "GAM Changer"
  },
  {
    "objectID": "dagnammit.html#causal-discovery",
    "href": "dagnammit.html#causal-discovery",
    "title": "DAG Nammit",
    "section": "Causal discovery? ü§î",
    "text": "Causal discovery? ü§î"
  },
  {
    "objectID": "dagnammit.html#key-takeaways",
    "href": "dagnammit.html#key-takeaways",
    "title": "DAG Nammit",
    "section": "üîë Key takeaways",
    "text": "üîë Key takeaways\n\nMachine learning models are great at identifying and utilizing patterns and associations in the data to make predictions\nCausal knowledge can be used to improve these models!\nSome quotes I like from Becoming A Data Head:\n\n\n\n\n‚ÄúThere are clever ways to use observational data to suggest some causal relationships. [They ALL] rely on strong assumptions and clever statistics.‚Äù\n\n\n‚ÄúAny claims of causality with observational data should be met with skeptimicism.‚Äù [(ANY!!)]"
  },
  {
    "objectID": "dagnammit.html#questions",
    "href": "dagnammit.html#questions",
    "title": "DAG Nammit",
    "section": "Questions? üôã",
    "text": "Questions? üôã\n\nSource: xkcd comic"
  }
]